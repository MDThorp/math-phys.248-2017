{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats with the VicPD crime data\n",
    "\n",
    "* Today we will explore the VicPD crime data file\n",
    "* Load it into Python memory\n",
    "* Start exploring some basic features of the data\n",
    "* Apply some basic statistics\n",
    "\n",
    "The <a href=\"https://vicpd.ca/crime-reports\">Victoria Police Department Crime Statistics</a> page provides an excellet data resource.  You will want to click on the VicPD badge and then set the filter so that you see some crime data.  \n",
    "\n",
    "**EDIT** I have included my download of the VicPD crime stats in the course repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json as js\n",
    "import pprint as pp\n",
    "\n",
    "with open('../data/vic_crimereports.json') as data_file:    \n",
    "    pdata = js.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(pdata.keys())\n",
    "#pp.pprint(pdata['meta'])\n",
    "print(\"There are \",len(pdata['data']), \" records.\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be careful!\n",
    "\n",
    "This is a very large file.  As we saw in Monday's class a simple command like **print pdata['data']** resulted in your instructor's poor laptop freezing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in range(28):\n",
    "    print(pdata['meta']['view']['columns'][x]['name'], \" \", end='')\n",
    "print('\\n')\n",
    "## decide what information to store, and how to store it. \n",
    "## we will use a namedtuple type, as it has a low memory profile\n",
    "#print(len(pdata['data']),'\\n')\n",
    "\n",
    "## reverse-lookup for defining our reduced data set. \n",
    "RLU = dict([(pdata['meta']['view']['columns'][x]['name'], x) for x in range(28)])\n",
    "#print(RLU, '\\n')\n",
    "\n",
    "keepFields = ['latitude', 'longitude', 'incident_type_primary', 'case_number', 'incident_datetime',\\\n",
    "             'address_1', 'created_at', 'updated_at', 'parent_incident_type']\n",
    "\n",
    "for x in keepFields:\n",
    "    print(x, \" \", pdata['data'][0][RLU[x]], \" \", sep='')\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "pdatt = namedtuple('pdatt', keepFields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "def isfloat(value):\n",
    "  try:\n",
    "    float(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "cdata = []\n",
    "for x in pdata['data']:\n",
    "    ## convert to dictionary\n",
    "    tdict = dict([(y, x[RLU[y]]) for y in keepFields])\n",
    "    \n",
    "    ## check to see if any terms undefined\n",
    "    nexists = False\n",
    "    for key, value in tdict.items():\n",
    "        if value==None:\n",
    "            nexists = True\n",
    "    ## let's ignore the records with no location data. \n",
    "    if nexists==True: \n",
    "        continue\n",
    "            \n",
    "    ## convert the numbers to floats\n",
    "    if isfloat(tdict['latitude']):\n",
    "        tdict['latitude'] = float(tdict['latitude'])\n",
    "    else:\n",
    "        print(\"Invalid lat.\")\n",
    "        continue\n",
    "    if isfloat(tdict['longitude']):\n",
    "        tdict['longitude'] = float(tdict['longitude'])\n",
    "    else:\n",
    "        print(\"Invalid long.\")\n",
    "        continue\n",
    "    \n",
    "    ## and the dates to python datetime objects\n",
    "    tdict['incident_datetime'] = dt.datetime.strptime(tdict['incident_datetime'],\\\n",
    "                                            '%Y-%m-%dT%H:%M:%S')\n",
    "    tdict['created_at'] = dt.datetime.strptime(tdict['created_at'],\\\n",
    "                                            '%Y-%m-%dT%H:%M:%S')\n",
    "    tdict['updated_at'] = dt.datetime.strptime(tdict['updated_at'],\\\n",
    "                                            '%Y-%m-%dT%H:%M:%S')\n",
    "   \n",
    "    ## convert dict to pdatt\n",
    "    pdat = pdatt(**tdict)\n",
    "    cdata.append(pdat)\n",
    "\n",
    "#print(len(cdata))\n",
    "print(cdata[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import attrgetter\n",
    "\n",
    "\n",
    "## let's get the earliest and most recent records, respectively. \n",
    "date_cdate = sorted(cdata, key = attrgetter('incident_datetime'))\n",
    "\n",
    "print(date_cdate[0], '\\n')\n",
    "print(date_cdate[-1], '\\n')\n",
    "## woot!  6 years of data!\n",
    "\n",
    "print(date_cdate[-1].incident_datetime - date_cdate[0].incident_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "## that's about 5 years, 150 days of data!\n",
    "\n",
    "## let's tabulate the \"Crime Types\" as a structure. \n",
    "\n",
    "ctypes = defaultdict(set)\n",
    "for x in cdata:\n",
    "    if x.parent_incident_type in ctypes.keys():\n",
    "        ctypes[x.parent_incident_type].add(x.incident_type_primary)\n",
    "    else:\n",
    "        ctypes[x.parent_incident_type] = set([x.incident_type_primary])\n",
    "        \n",
    "pp.pprint(ctypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try a heat map with a map of the city\n",
    "\n",
    "For this you will need to have **Folium** installed.  In your Virtual Machine (VM), at the terminal type **sudo pip install folium** if the \"import\" call below throws an error. \n",
    "\n",
    "*WENDI* at present does not have Folium installed, so this will only work on your VM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "## you might need to tell Python where your folium library is.  On my machine, external libraries\n",
    "## are installed in two locations. \n",
    "\n",
    "## these are where my non-anaconda python libraries are located.  I need these for\n",
    "## folium and such\n",
    "expaths = [\"/usr/lib/python3/dist-packages\", \"/usr/local/lib/python3.5/dist-packages\"]\n",
    "for xp in expaths:\n",
    "    if (xp not in sys.path):\n",
    "        sys.path.append(xp)\n",
    "\n",
    "import folium as fo\n",
    "from folium import plugins as fpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdata = []\n",
    "for x in cdata:\n",
    "    if (x.parent_incident_type==\"Assault with Deadly Weapon\"):\n",
    "        newpt = [x.latitude, x.longitude, 0.02 ] ## the last argument is the \"heat\"\n",
    "        hdata.append( newpt )\n",
    "mapa = fo.Map([48.4323, -123.3720], tiles='Stamen Terrain', zoom_start=13)\n",
    "mapa.add_children(fpl.HeatMap(hdata))\n",
    "#mapa.create_map(path='assault.wdw.heatmap.html')\n",
    "mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Heatmap, drugs on Fridays\n",
    "##  datetime weekday(), 0=Mon, 1=Tues, 2=Wed, 3=Thur, 4=Fri, 5=Sat, 6=Sun. \n",
    "hdata = []\n",
    "for x in cdata:\n",
    "    if (x.parent_incident_type==\"Drugs\") and\\\n",
    "       (x.incident_datetime.weekday()==4):\n",
    "        newpt = [x.latitude, x.longitude, 0.02 ] ## the last argument is the \"heat\"\n",
    "        hdata.append( newpt )\n",
    "mapa = fo.Map([48.4323, -123.3720], tiles='Stamen Terrain', zoom_start=13)\n",
    "mapa.add_children(fpl.HeatMap(hdata))\n",
    "#mapa.create_map(path='drugs.fri.heatmap.html')\n",
    "mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Pie chart of all the crime types. \n",
    "## Let's first do it on the \"parent\" crime type\n",
    "\n",
    "cnames = [p for p, q in ctypes.items()]\n",
    "\n",
    "tots = defaultdict(int)\n",
    "for x in cdata:\n",
    "    tots[x.parent_incident_type]+=1\n",
    "tot = 0\n",
    "for x,y in tots.items():\n",
    "    tot += y\n",
    "\n",
    "fractions = [100*y/tot for x,y in tots.items()]\n",
    "names = [x for x in tots.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.xkcd(): ## this enables the xkcd style.\n",
    "    \n",
    "    fig=plt.figure()\n",
    "    fig.set_size_inches(10,10) \n",
    "        \n",
    "    plt.pie(fractions, labels=names, autopct='%1.1f%%', shadow=False)\n",
    "    plt.title('Relative frequency of incident types', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## that's a little ugly, let's sort is so that small incidents are only beside \"big\" ones. . .\n",
    "## so we will sort the fractions on size, then re-shuffle them together. . .\n",
    "from operator import itemgetter\n",
    "\n",
    "dpairs = [(cnames[i], fractions[i]) for i in range(len(cnames))]\n",
    "sdpairs = sorted(dpairs, key=itemgetter(1))\n",
    "\n",
    "nnames = []\n",
    "nfracs = []\n",
    "\n",
    "while (len(cnames)>0):\n",
    "    nnames.append(cnames.pop(0) if (len(cnames) % 2 == 0) else cnames.pop())\n",
    "    nfracs.append(fractions.pop(0) if (len(fractions) % 2 == 0) else fractions.pop())\n",
    "## build the new names and fractions by taking the first and last elements of these lists, then popping them off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## same thing, but with nnames and nfracs\n",
    "with plt.xkcd(): ## this enables the xkcd style.\n",
    "    \n",
    "    fig=plt.figure()\n",
    "    fig.set_size_inches(10,10) \n",
    "        \n",
    "    plt.pie(nfracs, labels=nnames, autopct='%1.1f%%', shadow=False)\n",
    "    plt.title('Relative frequency of incident types', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's do a more detailed frequency analysis of the crime types.  Let's store the frequencies \n",
    "## for the parent types in a dict, and similarly store frequencies for the incident_type_primary\n",
    "##  as a dict. \n",
    "\n",
    "tot = 0\n",
    "all_tots = defaultdict(int)\n",
    "for x in cdata:\n",
    "    tot += 1\n",
    "    all_tots[x.parent_incident_type] += 1\n",
    "    all_tots[(x.parent_incident_type, x.incident_type_primary)] += 1\n",
    "#pp.pprint(all_tots)\n",
    "\n",
    "## compute the parent incident type frequencies\n",
    "all_freq = defaultdict(float)\n",
    "\n",
    "for x in ctypes.keys():\n",
    "    all_freq[x] = 100*all_tots[x] / tot\n",
    "    for y in ctypes[x]:\n",
    "        all_freq[(x,y)] = 100*all_tots[(x,y)] / all_tots[x]\n",
    "\n",
    "#pp.pprint(all_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## now let's list the major parent_incident_type items (say, over 10%)\n",
    "\n",
    "## and the major  incident_type_primary\n",
    "threshold = 5.0\n",
    "\n",
    "for x in ctypes.keys():\n",
    "    if all_freq[x]>threshold:\n",
    "        print(x, \"%.1f\" % all_freq[x])\n",
    "        for y in ctypes[x]:\n",
    "            if all_freq[(x,y)]>threshold:\n",
    "                print(\" -- \", y, \"%.1f\" % all_freq[(x,y)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## let's write something that counts the occurances of a crime type by the day of the week\n",
    "## crtype can be a string, or a pair of strings as in the above code\n",
    "def weekdaycount(crtype):\n",
    "    daycount = [0]*7\n",
    "    if (isinstance(crtype, str)):\n",
    "        for x in cdata:\n",
    "            if x.parent_incident_type == crtype:\n",
    "                daycount[x.incident_datetime.weekday()] += 1\n",
    "    elif (isinstance(crtype, tuple)) and (len(crtype)==2) and\\\n",
    "         (isinstance(crtype[0], str)) and (isinstance(crtype[1], str)):\n",
    "        for x in cdata:\n",
    "            if x.parent_incident_type == crtype[0] and x.incident_type_primary == crtype[1]:\n",
    "                daycount[x.incident_datetime.weekday()] += 1\n",
    "    return daycount\n",
    "\n",
    "def weekdaypct(crtype):\n",
    "    wdk = weekdaycount(crtype)\n",
    "    T = all_tots[crtype]\n",
    "    return ['{:.1f}'.format(100*x/T) for x in wdk]\n",
    "\n",
    "def presentBDWeek(crtype):\n",
    "    retval = \"Mon, Tue, Wed, Thu, Fri, Sat, Sun\\n\";\n",
    "    retval += str(weekdaypct(crtype))\n",
    "    return retval\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in ctypes.keys():\n",
    "    print(x, ' (tot ', all_tots[x], ')', sep='')\n",
    "    print(presentBDWeek(\"Liquor\"), '\\n')\n",
    "    for y in ctypes[x]:\n",
    "        print(x, \" - \", y, ' (tot ', all_tots[(x,y)], ')', sep='')\n",
    "        print(presentBDWeek( (x, y) ), '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather and crime relations. . .\n",
    "\n",
    "We want to peek a little deeper into the data.  Let's consider the relationship between weather and crime. For example, we could perform a plot of the number of crime types (say, traffic violations) vs. average daily temperatures.  For this, we will need temperature data, going back to **2011**. \n",
    "\n",
    "As in [Week 3](../Week.3/Lecture.2.ipynb) we will want to download the appropriate data from the Stats Canada [Historical Weather Database](http://climate.weather.gc.ca/historical_data/search_historic_data_e.html).  We have done so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os as os\n",
    "import fnmatch as fn\n",
    "\n",
    "files = fn.filter(os.listdir('../data'), \"eng-daily*.csv\")\n",
    "\n",
    "## let's store the weather data as a dictionary. \n",
    "\n",
    "wdatlist = {}\n",
    "\n",
    "for f in files:\n",
    "    with open(\"../data/\"+f) as fo:\n",
    "        #print(\"loading \", f)\n",
    "        content = fo.readlines()\n",
    "        for j in range(26, len(content)):\n",
    "            ab = content[j].split(',')\n",
    "            for k in range(len(ab)):\n",
    "                ab[k] = ab[k].translate({ord(c): None for c in '\"'})\n",
    "            date = dt.date(int(ab[1]), int(ab[2]), int(ab[3]))\n",
    "            if len(ab[5])>0 and len(ab[7])>0 and len(ab[9])>0:\n",
    "                wdatlist[date] = (float(ab[5]), float(ab[7]), float(ab[9]))\n",
    "            \n",
    "## wdatlist[date] = (max, min, mean) temps\n",
    "print(len(wdatlist.keys()) // 365, \" years of data and \", len(wdatlist.keys()) % 365, \" days\", sep='')\n",
    "\n",
    "## let's find all the common dates with data, and put into one big array. \n",
    "\n",
    "## we have wdatlist dict dates -> triples\n",
    "## and cdata a list of cstruc objects, that have datetimes. . .\n",
    "## so we need to convert cdata to an object indexed by dates, containing counts of \n",
    "## everything that occured on those dates. \n",
    "comdates = []\n",
    "\n",
    "ccdata = defaultdict(int)\n",
    "for xd in cdata:\n",
    "    ## only record if we have weather data\n",
    "    if xd.incident_datetime.date() in wdatlist.keys():\n",
    "        comdates.append(xd.incident_datetime.date())\n",
    "        ccdata[(xd.incident_datetime.date(),xd.parent_incident_type, xd.incident_type_primary)] += 1\n",
    "\n",
    "## takes as input parent_incident_type and incident_type primary, \n",
    "## builds list x,y coordinates of weather data, counts of crimes. \n",
    "## k = 0, 1, 2 for max min mean temps\n",
    "def xyplot(pit, itp, k):\n",
    "    x = [wdatlist[date][k] for date in comdates]\n",
    "    y = [ccdata[(date, pit, itp)] for date in comdates]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y = xyplot('Disorder','CAUSE A DISTURBANCE', 0)\n",
    "\n",
    "plt.xlabel('max temp')\n",
    "plt.ylabel('disorder')\n",
    "plt.title('disorder citation counts per day vs. max temp')\n",
    "\n",
    "plt.plot( x,y, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## plot of mean temp (x-axis) vs traffic collisions\n",
    "\n",
    "x,y = xyplot('Traffic','COLLISION-DAMAGE OVER $1000', 1)\n",
    "\n",
    "plt.xlabel('min temp')\n",
    "plt.ylabel('num collision')\n",
    "plt.title('collision counts per day vs. min temp')\n",
    "\n",
    "plt.plot( x,y, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Analysis\n",
    "\n",
    "One might have expected there to be a stronger relationship between weather and traffic accidents.  \n",
    "\n",
    "This gives us a good question to ask, perhaps for the next lab or homework assignment.\n",
    "\n",
    "**Question:** Is there a relationship between weather and car accidents?  Perhaps we are not seeing it because we have *chosen* to look at the data in a way that does not bring true insight into the question?\n",
    "\n",
    "**What might we have done wrong?**\n",
    " - We are looking at accidents during day n, but the freezing temperatures are on night n.  Perhaps we should be looking at accidents in the morning of day n+1? \n",
    " - If its been below freezing for an entire day, most of the time one would expect the roads are clear and free of ice.  So max/min/mean temperatures might not be important *on their own*.  \n",
    " - Perhaps we should look for the scenerio where on day n it is above freezing, rains, and it freezes during the night, then consider accidents on (?morning?) day n+1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## this begs the question, what is the relation between max and min tempteratures? \n",
    "\n",
    "x = [wdatlist[date][1] for date in comdates] #min\n",
    "y = [wdatlist[date][0] for date in comdates] #max\n",
    "plt.xlabel('min temp (night)')\n",
    "plt.ylabel('max temp (day)')\n",
    "plt.title('max vs. min daily temperatures')\n",
    "plt.plot(x,y,'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  We appear to have a rough correllation!\n",
    "\n",
    "A technique to quantify correllation that we learn about in **Math 211** is called the *least squares* technique. \n",
    "\n",
    "The idea is that, given a situation like the above where one has a list of $x$ and $y$ coordinates, one could try to **fit** a trend-line to the data:\n",
    "\n",
    "$$y = b + ax$$\n",
    "\n",
    "As we can see from the above plot, no single line will be a perfect fit for all the data, but we can ask for a *best fit*. \n",
    "\n",
    "Given numbers $(a,b) \\in \\mathbb R^2$ the **individual error** of the fit for a data point $(x_i, y_i)$ is defined to be\n",
    "\n",
    "$$E_i = y(x_i) - y_i = b + ax_i - y_i$$\n",
    "\n",
    "The **total error** (squared) is\n",
    "\n",
    "$$E^2 = \\sum_i E_i^2 = \\sum_i (b + ax_i - y_i)^2$$\n",
    "\n",
    "We use squares because:\n",
    " - It is simple algebraically\n",
    " - If we used the first power it would not be a very useful concept\n",
    " - If we used the first power and absolute values, i.e.\n",
    "$$E = \\sum_i |b + ax_i - y_i|$$\n",
    "we do get a useful concept, although it is more effort to work with. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares\n",
    "\n",
    "The idea is to consider the error squared function as a real-valued function of the two real variables $(a,b)$, i.e. \n",
    "\n",
    "$$E^2 : \\mathbb R^2 \\to \\mathbb R$$\n",
    "\n",
    "Using calculus we can check this function has a unique minimum, and from calculus it occurs when \n",
    "\n",
    "$$\\frac{\\partial (E^2)}{\\partial a} = \\frac{\\partial (E^2)}{\\partial b} = 0$$\n",
    "\n",
    "which is\n",
    "\n",
    "$$2 \\sum_i (b + ax_i - y_i)x_i = 2 \\sum_i (b + ax_i - y_i) = 0$$\n",
    "\n",
    "a system of two linear equations in the two variables $(a,b)$. We can re-write it as:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$bn + a \\sum_i x_i = \\sum_i y_i$$\n",
    "\n",
    "$$b \\sum_i x_i + a \\sum_i x_i^2 = \\sum_i y_i x_i$$\n",
    "\n",
    "With $n$ the number of points in the data set.  Linear systems can be written as matrix equations. . .\n",
    "\n",
    "$$\\pmatrix{  n & \\sum_i x_i \\\\ \\sum_i x_i & \\sum_i x_i^2} \\pmatrix{ b \\\\ a} = \\pmatrix{ \\sum_i y_i x_i \\cr \\sum_i y_i} $$\n",
    "\n",
    "Although in priciple this is enough to answer the question (multiply by the inverse of the square matrix!) people typically observe that the matrix on the left is the product of two matrices more directly associated to the data.\n",
    "\n",
    "$$X = \\pmatrix{ 1 & 1 & 1 & \\cdots & 1 \\\\ x_1 & x_2 & x_3 & \\cdots & x_n }$$\n",
    "\n",
    "Notice \n",
    "$$ \\pmatrix{  n & \\sum_i x_i \\\\ \\sum_i x_i & \\sum_i x_i^2}= XX^T$$\n",
    "\n",
    "Our linear algebra problem has become\n",
    "\n",
    "$$XX^T \\pmatrix{ b \\\\ a} = X \\vec y$$\n",
    "where $$\\vec y = \\pmatrix{y_1 \\cr y_2 \\cr . \\cr . \\cr y_n}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to this problem is therefore \n",
    "\n",
    "$$\\pmatrix{b \\cr a} = (XX^T)^{-1} X \\vec y$$\n",
    "\n",
    "Let's code it up in Python. The **numpy** library has convenient matrix-algebra objects and routines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.matrix([[x[i]**j for i in range(len(x))] for j in range(2)])\n",
    "\n",
    "bavec = ((X*(X.T)).I)*X*(np.matrix(y).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [wdatlist[date][1] for date in comdates] #min\n",
    "y = [wdatlist[date][0] for date in comdates] #max\n",
    "plt.xlabel('min temp (night)')\n",
    "plt.ylabel('max temp (day)')\n",
    "plt.title('max vs. min daily temperatures')\n",
    "plt.plot(x,y,'ro', label='incidences')\n",
    "\n",
    "xd = np.linspace(-10.0, 20.0)\n",
    "yv = bavec[0,0] + bavec[1,0]*xd\n",
    "plt.plot(xd,yv,'b-', label='best linear fit y = %.1fx + %.1f' % (bavec[1,0], bavec[0,0]) )\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature of least squares is that it minimizes total $E^2$. We compute this minimal\n",
    "$$E = \\sqrt{E^2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math as ma\n",
    "\n",
    "## sum (b+axi-yi)^2\n",
    "E = ma.sqrt(sum([(bavec[0,0] + bavec[1,0]*x[i] - y[i])**2 for i in range(len(x))] ))\n",
    "print(\"minimal E = \", E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice:\n",
    "\n",
    "From this we can conclude that the maximum daily temperature $M$ in Victoria is **typically** \n",
    "$$M \\simeq 1.2 m + 7.7$$\n",
    "where $m$ is the minimum daily temperature.  i.e there is more variation on warm days, plus around a $7.7$ degree shift independent of the temperature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the average error among all our samples, to get a better sense for for what the $\\sqrt{E^2}$ computation above means. \n",
    "\n",
    "The average error is\n",
    "$$avg.E = \\frac{1}{n} \\sum_i |b + ax_i - y_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avE = sum([ abs(bavec[0,0] + bavec[1,0]*x[i] - y[i]) for i in range(len(x))])/len(x)\n",
    "print(\"avg.E = \", avE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So on *average* if one uses the\n",
    "$$ M \\simeq 1.2 m + 7.7$$\n",
    "formula, one will typically be wrong by about $2.2$ degrees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## how about the max error?\n",
    "print(\"Max error: \", max([abs(bavec[0,0] + bavec[1,0]*x[i] - y[i])\\\n",
    "                          for i in range(len(x))]))\n",
    "## the min error?\n",
    "print(\"Min error: \", min([abs(bavec[0,0] + bavec[1,0]*x[i] - y[i])\\\n",
    "                          for i in range(len(x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## percentage of samples with errors bigger than 3? etc. \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
