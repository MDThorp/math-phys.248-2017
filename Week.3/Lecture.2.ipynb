{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping\n",
    "\n",
    "In this class we will begin a web scraping project. We will return to the theme of web scraping later this semester, once we are comfortable with a wider collection of tools. \n",
    "\n",
    "Web scraping involves writing code to *obtain data from the internet*, and to *re-structure* it in a way that it is *useful*.  Depending on how *cooperative* your on-line sources are, there is a varying amount of work one needs to do, including various levels of automation. \n",
    "\n",
    "We will focus on some of the simplest web-scraping tasks here, in large part to practice Python programming techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of a problem\n",
    "\n",
    "In the summer of 2016, Professor X made a few small changes to his house.  He would like to know if his changes made any real difference in the energy-efficiency of his house.  The house is mostly heated with electric base-board heaters.  Towards this end, we have some data available:\n",
    "\n",
    "* <a href=\"https://www.bchydro.com/index.html\">BC Hydro</a> (Professor X's electricity supplier) provides energy usage data for its customers in downloadable spreadsheets. The data comes in various resolutions (hourly, daily, etc).  We have daily data in our directory, kindly supplied by Professor X, starting in 2015, going until January 2017. \n",
    "\n",
    "* Stats Canada similarly provides downloadable weather data on its <a href=\"http://climate.weather.gc.ca/historical_data/search_historic_data_e.html\">Historical Weather Database. </a>  The relevant data for Victoria has similarly been downloaded and put in this directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project involves three main steps. \n",
    "\n",
    "**Step 1**: Download the data.  This has been done for us, but as you can imagine, depending on the webpage this might take rather sophisticated techniques.  Web pages don't always give you their data in downloadable spreadsheets.  Sometimes they are hidden in menus, require authorization, require clicking in specific buttons that are rendered by specialized protocols (Javascript, Flash player, etc).  We will explore this step in more detail for *less cooperative* on-line sources.  For this project, the data has kindly been supplied. \n",
    "\n",
    "**Step 2**: Assemble the data into a relevant format we can use for analysis.  \n",
    "\n",
    "**Step 3**: Do some analysis.  We will make a few plots, and consider a few variations on how we choose to look at our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2, part (a) -- the weather data\n",
    "\n",
    "We will store the weather data as Python **lists** *[time/date, weather data]* which we can sort on the time/date field. . . This allows us to not worry very much about the order the data is loaded into memory. \n",
    "\n",
    "The spreadsheets **.csv** files, these are <a href=\"https://en.wikipedia.org/wiki/ASCII\">ASCII</a> files, that can be read in any text editor.  \n",
    "\n",
    "* As we can see from inspection, there are several initial lines that give some context to the spreadsheet.  \n",
    "\n",
    "* There is a line which indicates what kind of data is in each column.  \n",
    "\n",
    "* The data appears, one line at a time.  Each column is separated by a comma. \n",
    "\n",
    "* Each row is separated by a <a href=\"https://en.wikipedia.org/wiki/Carriage_return\">carriage return</a>.  \n",
    "\n",
    "* The csv files all have the form eng-daily*.csv so this is how we will load them. \n",
    "\n",
    "For the time/date information, Python supplies the convenient *datetime* library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os as os\n",
    "import fnmatch as fn\n",
    "\n",
    "files = fn.filter(os.listdir('../data'), \"eng-daily*.csv\")\n",
    "## iteratively load the files, grab and parse lines. . .\n",
    "wdatlist = []\n",
    "for i in range(len(files)):\n",
    "    ## we need to add the encoding and errors flag because WENDI can't\n",
    "    ## parse unicode characters. \n",
    "    with open(\"../data/\"+files[i], encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.readlines()\n",
    "        ## in the daily spreadsheets line 25 is the descriptions, 26+ is the items.\n",
    "        ## in the hourly spreadsheets line 16 and 17+\n",
    "        for j in range(26, len(content)):\n",
    "            ab = content[j].split(\",\")\n",
    "            for k in range(len(ab)): # this removes the quotes around the numbers.\n",
    "                ab[k] = ab[k].translate({ord(c): None for c in '\"'})\n",
    "                ## this code choice is made so that we can remove characters from\n",
    "                ## unicode strings -- unicode is an increasingly popular text file\n",
    "                ## format. \n",
    "            date = dt.date(int(ab[1]), int(ab[2]), int(ab[3])) ## get date.\n",
    "            ## throw out days where we lack the relevant data for. . .\n",
    "            if (len(ab[5])>0 and len(ab[7])>0 and len(ab[9])>0):\n",
    "                wdatlist.append([date, float(ab[5]), float(ab[7]), float(ab[9])])\n",
    "\n",
    "#let's sort it on the date, just in case we loaded the files in the wrong order.\n",
    "from operator import itemgetter\n",
    "wdatlist = sorted(wdatlist, key=itemgetter(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## A simple plot to get a sense for the data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib nbagg\n",
    "plt.close()\n",
    "\n",
    "## let's make the x-axis the date, \n",
    "## and the        y-axis will have max, min and average temperatures.\n",
    "\n",
    "x = [p[0] for p in wdatlist]\n",
    "y1 = [p[1] for p in wdatlist] # max\n",
    "y2 = [p[2] for p in wdatlist] # min\n",
    "y3 = [p[3] for p in wdatlist] # mean\n",
    "\n",
    "plt.plot(x,y1, 'ro', label = \"max temp\")\n",
    "plt.plot(x,y2, 'yo', label = \"min temp\")\n",
    "plt.plot(x,y3, 'bo', label = \"mean temp\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2, part (b) -- Energy Data\n",
    "\n",
    "Now we do the same for the energy data. \n",
    "\n",
    "* The description of the columns is on line 4. \n",
    "* Data starts on line 5+. \n",
    "* Date format is Y/M/D, column 1. \n",
    "* Energy consumption is kWh, column 2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = fn.filter(os.listdir('./'), \"BCHydro.daily*.csv\")\n",
    "edatlist = []\n",
    "for i in range(len(files)):\n",
    "    with open(\"./\"+files[i]) as f:\n",
    "        content = f.readlines()\n",
    "        for j in range(5, len(content)):\n",
    "            ab = content[j].split(\",\")\n",
    "            for k in range(len(ab)):\n",
    "                ab[k] = ab[k].translate({ord(c): None for c in '\"'})\n",
    "            date = dt.datetime.strptime(ab[1],'%Y/%m/%d %H:%M:%S').date()\n",
    "            if (ab[2]!='N/A'):\n",
    "                edatlist.append([date, float(ab[2])])\n",
    "\n",
    "## sort edatlist, to ensure data is in historical order. \n",
    "edatlist = sorted(edatlist, key=itemgetter(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.close() # remember to close previous plot\n",
    "\n",
    "x = [p[0] for p in edatlist]\n",
    "y = [p[1] for p in edatlist] # max\n",
    "\n",
    "plt.plot(x,y, 'bo', label = \"kWh usage\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Basic analysis of the data\n",
    "\n",
    "Power consumption appears (sometimes) larger in the 2016-2017 winter compared with 2015-2016, but there are also colder days.  Let's try to compare power usage for similar-temperature days. For this we will merge the data sets into one *bdatlist*. \n",
    "\n",
    "* Want to make plots of temperatures vs. electricity costs for the two periods.  \n",
    "* We can make the x-axis average temperature\n",
    "* The y-axis can be cost. \n",
    "* Let's break the colors up into:\n",
    "  - red for summer 2016 and earlier.\n",
    "  - blue for fall 2016 and later. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## step 1: find the dates we have both temperatures and kWh usage, put them into a big list\n",
    "##         [date, max temp, min temp, mean temp, kWh usage]\n",
    "##\n",
    "##  we will run through wdatlist, and look for the date in edatlist\n",
    "\n",
    "bdatlist = []\n",
    "for x in wdatlist:\n",
    "    res = [element for element in edatlist if element[0] == x[0]]\n",
    "    if (len(res)>0):\n",
    "        bdatlist.append([x[0], x[1], x[2], x[3], res[0][1]])\n",
    "\n",
    "bdatlist0 = []\n",
    "bdatlist1 = []\n",
    "for x in bdatlist:\n",
    "    if (x[0] < dt.date(2016,6,1)):\n",
    "        bdatlist0.append(x)\n",
    "    else:\n",
    "        bdatlist1.append(x)\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib nbagg\n",
    "\n",
    "## we need the x and y coordinates as lists, for the <2016 data\n",
    "## and the x and y coordinates for the >2016 data. \n",
    "x1 = [bdatlist0[i][2] for i in range(len(bdatlist0))]\n",
    "y1 = [bdatlist0[i][4] for i in range(len(bdatlist0))]\n",
    "\n",
    "x2 = [bdatlist1[i][2] for i in range(len(bdatlist1))]\n",
    "y2 = [bdatlist1[i][4] for i in range(len(bdatlist1))]\n",
    "\n",
    "plt.plot(x1,y1, 'ro', label=\"before summer 2016\")\n",
    "plt.plot(x2,y2, 'bo', label=\"summer 2016 to Jan 2017\")\n",
    "plt.title(\"kWh electricity usage vs. min temperature\")\n",
    "plt.xlabel(\"min daily temperature\")\n",
    "plt.ylabel(\"kWh electricity consumption\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 3 continued: other plots to consider? \n",
    "\n",
    "* Weekends vs non-weekends? \n",
    "* kWh averages for given temperatures? \n",
    "* Use average temperatures rather than min? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Let's try computing kWh averages for every degree c, using rounding\n",
    "plt.close()\n",
    "\n",
    "## start by building a list: [(temp, [energy use readings])]\n",
    "##  then we compute averages. \n",
    "import collections as co\n",
    "\n",
    "table5m = co.defaultdict(list) ## min temps and energy usage, 2015-2016\n",
    "table5M = co.defaultdict(list) ## mean temps and energy usage, 2015-2016\n",
    "table6m = co.defaultdict(list) ## min temps and energy usage 2016-2017\n",
    "table6M = co.defaultdict(list) ## mean temps and energy usage, 2016-2017\n",
    "for b in bdatlist0:\n",
    "    ## b = [date, max, min, mean, Euse]\n",
    "    table5m[round(b[2])].append(b[4])\n",
    "    table5M[round(b[3])].append(b[4])\n",
    "for b in bdatlist1:\n",
    "    table6m[round(b[2])].append(b[4])\n",
    "    table6M[round(b[3])].append(b[4])\n",
    "    \n",
    "avg5m = co.defaultdict(float)\n",
    "avg5M = co.defaultdict(float)\n",
    "avg6m = co.defaultdict(float)\n",
    "avg6M = co.defaultdict(float)\n",
    "for x in table5m:\n",
    "    avg5m[x] = sum(table5m[x])/len(table5m[x])\n",
    "for x in table5M:\n",
    "    avg5M[x] = sum(table5M[x])/len(table5M[x])\n",
    "for x in table6m:\n",
    "    avg6m[x] = sum(table6m[x])/len(table6m[x])\n",
    "for x in table6M:\n",
    "    avg6M[x] = sum(table6M[x])/len(table6M[x])\n",
    "\n",
    "## we need to sort lowest to highest (temperature)\n",
    "## otherwise the plot will not satisfy the vertical line rule\n",
    "avg5m = co.OrderedDict(sorted(avg5m.items()))\n",
    "avg5M = co.OrderedDict(sorted(avg5M.items()))\n",
    "avg6m = co.OrderedDict(sorted(avg6m.items()))\n",
    "avg6M = co.OrderedDict(sorted(avg6M.items()))\n",
    "\n",
    "x5m = [x for x in avg5m]\n",
    "y5m = [avg5m[x] for x in avg5m]\n",
    "x5M = [x for x in avg5M]\n",
    "y5M = [avg5M[x] for x in avg5M]\n",
    "\n",
    "x6m = [x for x in avg6m]\n",
    "y6m = [avg6m[x] for x in avg6m]\n",
    "x6M = [x for x in avg6M]\n",
    "y6M = [avg6M[x] for x in avg6M]\n",
    "\n",
    "plt.plot(x5m, y5m, 'y-', label='2015-2016 (min T)')\n",
    "#plt.plot(x5M, y5M, 'r-', label='2015-2016 (mean T)')\n",
    "plt.plot(x6m, y6m, 'b-', label='2016-2017 (min T)')\n",
    "#plt.plot(x6M, y6M, 'g-', label='2016-2017 (mean T)')\n",
    "plt.xlabel('min temperature')\n",
    "plt.ylabel('average energy usage on those days')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated web scraping\n",
    "\n",
    "Let's consider some basic data-harvesting problems that could use automation. \n",
    "\n",
    "* The <a href=\"https://developers.google.com/maps/documentation/elevation/intro\">Google elevation API</a> is a web-interface set up by Google to give elevation data for any coordinate on earth (in metres).  We will attempt some basic web-scraping of the Google page. \n",
    "\n",
    "* The <a href=\"https://www.wunderground.com/weather/api/\">Weather Underground API</a> allows automated exploration of weather data from hundreds of thousands of weather stations all over the planet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Scraping the Google elevation website\n",
    "\n",
    "The google website is one of the simplest to scrape.  We provide the website with our request on the *https* request.  It provides us with a custom website, basically a text file, in response. \n",
    "\n",
    "* The response is in <a href=\"http://www.json.org/\">JSON</a> a standard structured text-file format.  \n",
    "\n",
    "* Although JSON is human-readable, it is also very closely related to the Python *dict* and *list* data types.  The **json** library allows for easy conversion between JSON files and Python objects. \n",
    "\n",
    "* We will use the *urllib* library to access the internet.  This is good accessing data on basic *http* webpages.  Webpages that use further enhancements like <a href=\"https://www.javascript.com/\">Javascript</a> and <a href=\"https://get.adobe.com/flashplayer/?no_redirect\">Flash</a> require more sophisticated web-scraping tools that we will explore later in the course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json as js ## many web pages provide data in the JSON format, this helps us read it.\n",
    "from urllib.request import urlopen ## URLlib is one of the most \"basic\" libraries for accessing webpages.\n",
    "import time as ti ## Web pages tend to block users that ask for too much data too quickly.  This library\n",
    " ## lets us put delays into our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Let's request from Google the elevation of the roof in the building we're in. \n",
    "\n",
    "f=urlopen(\"https://maps.googleapis.com/maps/api/elevation/json?\"\n",
    "                  \"locations=48.464958,-123.313418\")\n",
    "#print(f.read())\n",
    "json_string = f.read().decode('utf-8')\n",
    "print(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jsl = js.loads(json_string)\n",
    "print(type(jsl))\n",
    "print(jsl)\n",
    "print(jsl['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Let's request the elevation of Mount Everest.  We find its longitude and latitude from Wikipedia\n",
    "\n",
    "## 27°59′17″N 86°55′31″E --> 27.9881° N, 86.9253° E converted. \n",
    "\n",
    "f=urlopen(\"https://maps.googleapis.com/maps/api/elevation/json?\"\n",
    "          \"locations=27.9881,86.9253\")\n",
    "json_string = f.read().decode('utf-8')\n",
    "jsl = js.loads(json_string)\n",
    "print(jsl['results'][0]['elevation'], 'metres')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Weather Underground\n",
    "\n",
    "<a href=\"https://www.wunderground.com/\">**Weather Underground**</a> is a webpage for a network of home weather enthusiasts on the internet.  There are hundreds of thousands of home <a href=\"https://www.amazon.com/s?field-keywords=weather+station\">weather stations</a> around the planet, connected to the internet reporting real-time weather data. \n",
    "\n",
    "In this code we will do some very elementary web scraping from Weather Underground.  **Weather Underground** is a heavily-mined website by people's cell phones, internet companies, news stations, etc.  So they put restrictions on  the amount (and frequency) of access.  This website demands users register and get an **API Key** to access the site. How to get an API is described <a href=\"https://www.wunderground.com/weather/api/d/docs\">here</a> as well as how to format one's *http* requests. \n",
    "\n",
    "If you want to execute this code, you will need to generate a Weather Underground API key. Go to the\n",
    "<a href=\"https://www.wunderground.com/weather/api/\">Weather Underground API \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## to complete this project you will have to \n",
    "import wundergroundkey as wk\n",
    "\n",
    "## \"WeatherStructure\" is the name of this data type. \n",
    "## If X is a WStruct, you can call its attributes by: x.StationID, x.assoc, etc. \n",
    "WStruct = co.namedtuple(\"WeatherStructure\", \"StationID assoc neighbourhood city province country lat lon elev time weather temp_c humid wind_string wind_dir wind_deg wind_kph pressure_mb dewpt_c precip_today\")\n",
    "\n",
    "## Give this routine a station ID, it will return a WStruct associated to the station\n",
    "def FetchWeather(fStationID):\n",
    "    URL_string = \"http://api.wunderground.com/api/\"+wk.WAPI+\"/geolookup/conditions/q/pws:\"+fStationID+\".json\"\n",
    "    #print(fStationID, end=\"\", flush=True)\n",
    "    URLobj = urlopen(URL_string)\n",
    "    #print(\". \",end=\"\", flush=True)\n",
    "    json_string = URLobj.read().decode('utf-8')\n",
    "    parsed_json = js.loads(json_string) ## This creates an index for reading the file\n",
    "    \n",
    "    # We will use this dictionary-object to build the retval, a WStruct type. \n",
    "    adj_pws_stations = parsed_json['location']['nearby_weather_stations']['pws']['station']\n",
    "    nearby_stations = [ adj_pws_stations[i]['id'] for i in range(len(adj_pws_stations)) ]\n",
    "    \n",
    "    # Return the WStruct\n",
    "    return WStruct(StationID = fStationID, assoc = nearby_stations, neighbourhood = \\\n",
    "                   adj_pws_stations[0]['neighborhood'], \\\n",
    "                   city = parsed_json['current_observation']['display_location']['city'], \\\n",
    "                   province = parsed_json['current_observation']['display_location']['state_name'], \\\n",
    "                   country = parsed_json['current_observation']['display_location']['country'], \\\n",
    "                   lat = parsed_json['current_observation']['display_location']['latitude'], \\\n",
    "                   lon = parsed_json['current_observation']['display_location']['longitude'], \\\n",
    "                   elev = parsed_json['current_observation']['display_location']['elevation'], \\\n",
    "                   time = parsed_json['current_observation']['observation_time'], \\\n",
    "                   weather = parsed_json['current_observation']['weather'], \\\n",
    "                   temp_c = parsed_json['current_observation']['temp_c'], \\\n",
    "                   humid = parsed_json['current_observation']['relative_humidity'], \\\n",
    "                   wind_string = parsed_json['current_observation']['wind_string'],\\\n",
    "                   wind_dir = parsed_json['current_observation']['wind_dir'], \\\n",
    "                   wind_deg = parsed_json['current_observation']['wind_degrees'], \\\n",
    "                   wind_kph = parsed_json['current_observation']['wind_kph'], \\\n",
    "                   pressure_mb = parsed_json['current_observation']['pressure_mb'], \\\n",
    "                   dewpt_c = parsed_json['current_observation']['dewpoint_c'], \\\n",
    "                   precip_today = parsed_json['current_observation']['precip_today_metric'])\n",
    "\n",
    "f = FetchWeather(\"IBRITISH479\")\n",
    "print(f.weather, \", \", f.temp_c, \" deg c\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment 1\n",
    "\n",
    "Your first homework assignment builds on the first few lectures, and our Week 2 and Week 3 labs. It is available [here](../Homework/asst.1.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
