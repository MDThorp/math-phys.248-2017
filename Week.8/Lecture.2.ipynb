{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime stats and matplotlib\n",
    "\n",
    "This last lecture will start with an overview of **matplotlib**, using our crime and weather data as a source of information to plot. \n",
    "\n",
    "## Plotting in two dimensions with matplotlib.\n",
    "\n",
    "A useful resource for the basics of matplotlib is the [matplotlib FAQ \"General Concepts\" section](http://matplotlib.org/devdocs/faq/usage_faq.html#general-concepts).  It outlines the primary structures and terminology used by matplotlib.   A more encyclopedic (but very good) introduction is [available here](http://www.labri.fr/perso/nrougier/teaching/matplotlib/).  We give a quick summary of matplotlib below. \n",
    "\n",
    "The **matplotlib.pyplot** module is the core library for producing 2-dimensional plots in matplotlib.  A display produced by matplotlib is called a **figure**, and figures have potentially many parts, called **axes**. \n",
    "\n",
    "<img src=\"../images/fig_map.png\" alt=\"Riemann sum example\" width=300 height=300 alt=\"taken from http://matplotlib.org/devdocs/faq/usage_faq.html#general-concepts\">\n",
    "\n",
    "The **axes** belong to the figure.  matplotlib has a high-level graphics-display object called an **artist** and all objects (figures, axes, axis, text, etc) are artist objects. \n",
    "\n",
    "matplotlib expects all array-objects to be **numpy** arrays.  Other array types can work in matplotlib but be aware these can create problems. \n",
    "\n",
    "matplotlib documentation distinguishes between the **backend** and the **frontend**. \n",
    "    \n",
    " - The **frontend** refers to the way in which you generate code for matplotlib.  For us, this is the i-python notebook.  \n",
    " - The **backend** refers to how one turns the code into graphics, or potentially an interactive environment. \n",
    "    \n",
    "There are two primary backend types for matplotlib:\n",
    "\n",
    " - Hardcopy backends.  These generate static image files from your code. The code **%matplotlib inline** loads a standard static backend for matplotlib.\n",
    " - Interactive backends.  These generate code (some generate and execute the code) for interactive graphics.  For example, some backends generate java code that can be integrated with a web-page to render your application on-line. The code **%matplotlib nbagg** loads a standard interactive back-end for matplotlib.\n",
    " - A [list](http://matplotlib.org/faq/usage_faq.html#what-is-a-backend) of preloaded matplotlib backends is here. \n",
    "        \n",
    "For most tasks we will use the default backends (inline and nbagg) for matplotlib.  This requires no special actions. We will also explore applications that use other backends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple plot types with matplotlib\n",
    "\n",
    "Let's start off making a basic figure. Let's plot the max and minimum temperatures for a range of days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import vicpd as vpd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# order the weather data!\n",
    "from collections import OrderedDict\n",
    "\n",
    "owdat = OrderedDict(sorted(vpd.wdatlist.items()))\n",
    "\n",
    "t = [k for k,v in owdat.items()][50:62]\n",
    "y1 = [v[0] for k,v in owdat.items()][50:62]\n",
    "plt.plot(t,y1, label=\"max temps (c)\")\n",
    "\n",
    "y2 = [v[1] for k,v in owdat.items()][50:62]\n",
    "plt.plot(t,y2, label=\"min temps (c)\")\n",
    "\n",
    "y3 = [v[2] for k,v in owdat.items()][50:62]\n",
    "plt.plot(t,y3, label=\"mean temps (c)\")\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('temperatures')\n",
    "plt.title('Daily max, min, mean temperatures')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criticisms:\n",
    "\n",
    " - The text on the x-axis is difficult to read.  We could use smoother fonts, such as **LaTeX** rendering, or simply enlarge the font.  If you get an error code when using the usetex command, check the error message.  It likely means you are missing an application to convert dvi files to png files.  A quick **sudo apt install dvipng** command should fix that. \n",
    " - Figure could be larger. The **set_size_inches** command works for this. \n",
    " - The legend is obstructing the figure. The [legend locator](http://matplotlib.org/users/legend_guide.html) is good for this. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure().set_size_inches(10,10)\n",
    "plt.rc('text', usetex=True) ## smoother fonts!\n",
    "\n",
    "plt.plot(t,y1, label=\"max temps (c)\")\n",
    "plt.plot(t,y3, label=\"mean temps (c)\")\n",
    "plt.plot(t,y2, label=\"min temps (c)\")\n",
    "\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('temperatures')\n",
    "plt.title('Daily max, min, mean temperatures', fontsize=14)\n",
    "\n",
    "plt.tick_params(axis='both', labelsize=14) ## size of dates and numbers on axis\n",
    "\n",
    "locs, labels = plt.xticks() ## rotation of x-axis labels\n",
    "plt.setp(labels, rotation=60)\n",
    "\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit our plots of daily crime counts, one vs. the other, and try to make them more informative. Our original plots just had one dot for each day.  Let's start by plotting pairs **(num crime type x, num crime type y)** by days, over our full crime data set.  \n",
    "\n",
    "We create a command analogous to **vpd.xyplot** command, but where we use **ccdata** for both the **x** and **y** axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xyplot(pit1, itp1, pit2, itp2):\n",
    "    x = [vpd.ccdata[(date, pit1, itp1)] for date in vpd.comdates]\n",
    "    y = [vpd.ccdata[(date, pit2, itp2)] for date in vpd.comdates]\n",
    "    return x,y\n",
    "\n",
    "def makeccplot(pit1, itp1, pit2, itp2):\n",
    "    plt.xlabel( (pit1+\" \"+itp1).translate({ord(c): '\\$' for c in '$'}) )\n",
    "    plt.ylabel( (pit2+\" \"+itp2).translate({ord(c): '\\$' for c in '$'}) )\n",
    "    plt.title( \"Daily count comparison\" )\n",
    "    x,y = xyplot(pit1, itp1, pit2, itp2)\n",
    "    plt.plot(x,y,'ro', label='daily incidences')\n",
    "\n",
    "makeccplot('Traffic','COLLISION-DAMAGE OVER $1000',\\\n",
    "           'Traffic','COLLISION-DAMAGE UNDER $1000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this is not particularly informative, as some of these dots are likely printed multiple times.  \n",
    "\n",
    "We have a few options:\n",
    "\n",
    " - Make a dot larger if it occurs more often. \n",
    " - For each point on the x-axis plot the average of the y-axis values, and perhaps indicate the variation in y-values with error bars. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try the dot size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math as ma\n",
    "\n",
    "def xyrplot(pit1, itp1, pit2, itp2):\n",
    "    x = [vpd.ccdata[(date, pit1, itp1)] for date in vpd.comdates]\n",
    "    y = [vpd.ccdata[(date, pit2, itp2)] for date in vpd.comdates]\n",
    "    pairs = [(x[i], y[i]) for i in range(len(x))]\n",
    "    pcount = defaultdict(int)\n",
    "    for i in range(len(x)):\n",
    "        pcount[(x[i], y[i])] += 1\n",
    "    return pcount\n",
    "\n",
    "def makeccrplot(pit1, itp1, pit2, itp2):\n",
    "    plt.xlabel( (pit1+\" \"+itp1).translate({ord(c): '\\$' for c in '$'}) )\n",
    "    plt.ylabel( (pit2+\" \"+itp2).translate({ord(c): '\\$' for c in '$'}) )\n",
    "    plt.title( \"Daily count comparison\" )\n",
    "    xyr = xyrplot(pit1, itp1, pit2, itp2)\n",
    "    x = [d[0] for d,c in xyr.items()]\n",
    "    y = [d[1] for d,c in xyr.items()]\n",
    "    r = [ma.sqrt(c) for d,c in xyr.items()]\n",
    "    ## and to fix the positioning of the axis...\n",
    "    xd = (max(x)-min(x))*0.04\n",
    "    yd = (max(y)-min(y))*0.04\n",
    "    plt.axis([min(x)-xd, max(x)+xd, min(y)-yd, max(y)+yd])\n",
    "\n",
    "    plt.scatter(x,y, s=r, label='daily incidences')\n",
    "\n",
    "makeccrplot('Traffic','COLLISION-DAMAGE OVER $1000',\\\n",
    "            'Traffic','COLLISION-DAMAGE UNDER $1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "makeccrplot('Assault with Deadly Weapon', 'ASSAULT-W/WEAPON OR CBH',\\\n",
    "            'Weapons Offense', 'WEAPONS-POSSESSION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would indicate that no collisions of any type is the *default* and collisions over or under $\\$1000$ in damage are unrelated.\n",
    "\n",
    "Let's check if we would *see* the same relation by plotting averages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ccavyplot(pit1, itp1, pit2, itp2):\n",
    "    x = [vpd.ccdata[(date, pit1, itp1)] for date in vpd.comdates]\n",
    "    y = [vpd.ccdata[(date, pit2, itp2)] for date in vpd.comdates]\n",
    "    pairs = [(x[i], y[i]) for i in range(len(x))]\n",
    "    pcount = defaultdict(int)\n",
    "    psum = defaultdict(int)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        pcount[x[i]] += 1\n",
    "        psum[x[i]] += y[i]\n",
    "    avg = [pcount[xi]/psum[xi] for xi in psum.keys()]\n",
    "\n",
    "    dev = defaultdict(float)\n",
    "    for i in range(len(x)):\n",
    "        dev[x[i]] += abs(y[i]-avg[x[i]])\n",
    "    for i in dev.keys():\n",
    "        dev[i] /= pcount[i]\n",
    "    devl = [dev[i] for i in dev.keys()]\n",
    "    return list(psum.keys()), avg, devl\n",
    "\n",
    "## let's compute the deviation from the average as well, and put that\n",
    "## in as error bars. \n",
    "xv, yv, dv = ccavyplot('Traffic','COLLISION-DAMAGE OVER $1000',\\\n",
    "            'Traffic','COLLISION-DAMAGE UNDER $1000')\n",
    "plt.title('Average number of collisions under 1000, for a given\\n number of collisions over. With avg errors')\n",
    "plt.errorbar(xv, yv, yerr=dv, ecolor='r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of the averages look quite different.  Although, this is about what we would expect -- that there is no real trend, other than there being slightly more minor collisions than major collisions, on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## few assaults on snowy days\n",
    "xv, yv = vpd.xyplot('Assault', 'ASSAULT-COMMON OR TRESPASS', 4)\n",
    "plt.plot(xv, yv, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## few (but more) assaults on rainy days\n",
    "xv, yv = vpd.xyplot('Assault', 'ASSAULT-COMMON OR TRESPASS', 3)\n",
    "plt.plot(xv, yv, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Assaults vs. max temp\n",
    "xv, yv = vpd.xyplot('Assault', 'ASSAULT-COMMON OR TRESPASS', 0)\n",
    "plt.plot(xv, yv, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "makeccrplot('Other', 'SUSPICIOUS PERS/VEH/OCCURRENCE',\\\n",
    "            'Theft from Vehicle', 'THEFT FROM MV UNDER $5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "makeccrplot('Other', 'SUSPICIOUS PERS/VEH/OCCURRENCE',\\\n",
    "            'Liquor', 'LIQUOR-INTOX IN PUBLIC PLACE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Least squares again**\n",
    "\n",
    "Although we have not seen many examples where the data appears to fit a linear trend, we should mention that the least squares technique is more general than what we described in Week 6. \n",
    "\n",
    "Assume you have some data points $(x_i, y_i)$ for $i = 1, 2, \\cdots, n$.  And you want to find a *best* fit function of the form\n",
    "\n",
    "$$F(x) = \\sum_{i=1}^n c_i f_i(x)$$\n",
    "\n",
    "with the functions $f_i : \\mathbb R \\to \\mathbb R$ being given ahead of time. i.e. we want to interpolate our data as a linear combination of some functions that we have decided upon *ahead of time*.  In our example from [Week 6](../Week.6/Lecture.2.ipynb), we had $n=2$, with $f_1(x) = x$ and $f_2 = 1$. \n",
    "\n",
    "As in Week 6, given a choice of constants $\\{c_i : i = 1, 2, \\cdots, n\\}$ the **Total Error** (squared) of the approximation is defined as:\n",
    "\n",
    "$$E^2 = \\sum_{i=1}^n (F(x_i) - y_i)^2 $$\n",
    "\n",
    "Since $F$, $x_i$, and $y_i$ are given, we can think of $E^2$ as a function of the coefficients $(c_1, c_2, \\cdots, c_n)$.  By calculus the minimum occurs at a critical point, which is when:\n",
    "\n",
    "$$\\frac{\\partial E^2}{\\partial c_1} = \\frac{\\partial E^2}{\\partial c_2} = \\cdots = \\frac{\\partial E^2}{\\partial c_n} = 0 \\hskip 1cm \\star$$\n",
    "\n",
    "which is a system of $n$ linear equations in the $n$ variables $\\{c_1, c_2, \\cdots, c_n\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, \n",
    "\n",
    "$$\\frac{\\partial E^2}{\\partial c_k} = \\sum_{i=1}^n 2(F(x_i)-y_i)f_k(x_i), \\hskip 1cm k=1,2,\\cdots, n$$\n",
    "which allows us to express the linear system $\\star$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i,j=1}^n c_jf_j(x_i)f_k(x_i) = \\sum_{i=1}^n y_i f_k(x_i), \\hskip 1cm k=1,2,\\cdots,n$$\n",
    "which in turn is the matrix equation\n",
    "\n",
    "$$AA^T \\vec c = A \\vec y$$\n",
    "where \n",
    "$$A = \\pmatrix{f_1(x_1) & f_1(x_2) & \\cdots & f_1(x_n) \\cr f_2(x_1) & f_2(x_2) & \\cdots & f_2(x_n) \\cr . & . & & . \\cr  . & . & & . \\cr f_n(x_1) & f_n(x_2) & \\cdots & f_n(x_n)}$$\n",
    "$\\vec y = \\pmatrix{y_1 \\cr y_2 \\cr . \\cr . \\cr y_n}$, and \n",
    "$\\vec c = \\pmatrix{c_1 \\cr c_2 \\cr . \\cr . \\cr c_n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "Let's take the max vs. min temperature example from Week 6, but interpolate with more than linear functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## A reminder of the data. \n",
    "x = [vpd.wdatlist[date][1] for date in vpd.comdates] #min\n",
    "y = [vpd.wdatlist[date][0] for date in vpd.comdates] #max\n",
    "plt.xlabel('min temp (night)')\n",
    "plt.ylabel('max temp (day)')\n",
    "plt.title('max vs. min daily temperatures')\n",
    "plt.plot(x,y,'ro')\n",
    "\n",
    "## implementing a linear interpolation\n",
    "A = np.matrix([[x[i]**j for i in range(len(x))] for j in range(2)])\n",
    "cvec = ((A*(A.T)).I)*A*(np.matrix(y).T)\n",
    "\n",
    "xd = np.linspace(-10.0, 20.0)\n",
    "yd = cvec[0,0] + cvec[1,0]*xd\n",
    "plt.plot(xd, yd, 'b-')\n",
    "\n",
    "## and the average error.\n",
    "avE = sum([ abs(cvec[0,0] + cvec[1,0]*x[i] - y[i]) for i in range(len(x))])/len(x)\n",
    "print(\"average error = \", avE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## interpolate using a+bx+cx**2\n",
    "import numpy as np\n",
    "\n",
    "A = np.matrix([[x[i]**j for i in range(len(x))] for j in range(3)])\n",
    "\n",
    "cvec = ((A*(A.T)).I)*A*(np.matrix(y).T)\n",
    "\n",
    "plt.xlabel('min temp (night)')\n",
    "plt.ylabel('max temp (day)')\n",
    "plt.title('max vs. min daily temperatures')\n",
    "plt.plot(x,y,'ro', label='incidences')\n",
    "\n",
    "xd = np.linspace(-10.0, 20.0)\n",
    "yv = cvec[0,0] + cvec[1,0]*xd + cvec[2,0]*(xd**2)\n",
    "plt.plot(xd,yv,'b-', label='best fit y = %.2f$x^2$ + %.1f$x$ + %.1f' % (cvec[2,0], cvec[1,0], cvec[0,0]) )\n",
    "plt.legend()\n",
    "\n",
    "## and we should expect the total error squared to be smaller, but how about average error?\n",
    "avE = sum([ abs(cvec[0,0] + cvec[1,0]*x[i] + cvec[2,0]*(x[i]**2) - y[i]) for i in range(len(x))])/len(x)\n",
    "print(\"average error = \", avE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## lets try a more flexible interpolation, let's use 1, x, x^2, sin(x) and cos(x)\n",
    "flist = [lambda x: 1.0, lambda x: x, lambda x: x**2,\\\n",
    "         lambda x: np.sin(x), lambda x: np.cos(x)]\n",
    "fname = ['', '$x$', '$x^2$', '$\\sin(x)$', '$\\cos(x)$']\n",
    "\n",
    "A = np.matrix([[F(x[i]) for i in range(len(x))] for F in flist])\n",
    "\n",
    "cvec = ((A*(A.T)).I)*A*(np.matrix(y).T)\n",
    "\n",
    "plt.xlabel('min temp (night)')\n",
    "plt.ylabel('max temp (day)')\n",
    "plt.title('max vs. min daily temperatures')\n",
    "plt.plot(x,y,'ro', label='incidences')\n",
    "\n",
    "xd = np.linspace(-10.0, 20.0)\n",
    "yd = [sum([flist[j](xi)*cvec[j,0] for j in range(len(flist))]) for xi in xd]\n",
    "\n",
    "fitstring=''\n",
    "for i in range(len(flist)):\n",
    "    if i!=0: fitstring+=\"+\"\n",
    "    fitstring+=(\"%.2f\" % cvec[i,0])+fname[i]\n",
    "plt.plot(xd,yd,'b-', label='best fit '+fitstring )\n",
    "plt.legend()\n",
    "\n",
    "## and the average error.\n",
    "avE = sum([ abs(sum([flist[j](x[i])*cvec[j,0] for j in range(len(flist))]) - y[i]) for i in range(len(x))])/len(x)\n",
    "print(\"average error = \", avE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## just for kicks, lets try 1, sin(x/6), cos(x/6), sin(x/3), cos(x/3)\n",
    "\n",
    "flist = [lambda x: 1.0, lambda x: np.sin(x/6), lambda x: np.cos(x/6), lambda x: np.sin(x/3), lambda x: np.cos(x/3)]\n",
    "fname = ['', '$\\sin(x/6)$', '$\\cos(x/6)$', '$\\sin(x/3)$', '$\\cos(x/3)$']\n",
    "\n",
    "A = np.matrix([[F(x[i]) for i in range(len(x))] for F in flist])\n",
    "\n",
    "cvec = ((A*(A.T)).I)*A*(np.matrix(y).T)\n",
    "\n",
    "plt.xlabel('min temp (night)')\n",
    "plt.ylabel('max temp (day)')\n",
    "plt.title('max vs. min daily temperatures')\n",
    "plt.plot(x,y,'ro', label='incidences')\n",
    "\n",
    "xd = np.linspace(-10.0, 20.0)\n",
    "yd = [sum([flist[j](xi)*cvec[j,0] for j in range(len(flist))]) for xi in xd]\n",
    "\n",
    "fitstring=''\n",
    "for i in range(len(flist)):\n",
    "    if i!=0: fitstring+=\"+\"\n",
    "    fitstring+=(\"%.2f\" % cvec[i,0])+fname[i]\n",
    "plt.plot(xd,yd,'b-', label='best fit '+fitstring )\n",
    "plt.legend()\n",
    "\n",
    "## and the average error.\n",
    "avE = sum([ abs(sum([flist[j](x[i])*cvec[j,0] for j in range(len(flist))]) - y[i]) for i in range(len(x))])/len(x)\n",
    "print(\"average error = \", avE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least-squares as a technique is extremely flexible in fitting one set of variables $y_i$ to a linear combination of functions of another variable $x_i$.   One often wants to know **what** objects to compare, or if there is any interesting comparisons to make **at all**.  \n",
    "\n",
    "The technique of [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis), roughly speaking, attempts to find a *best possible* ellipsoidal shape fitting a collection of data. The data cloud can be a finite set of vectors in $\\mathbb R^n$, thus it allows the simultaneous comparison of many different aspects of a data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The PCA technique\n",
    "\n",
    "Rather than implement PCA from scratch, we will start with a library implementation. For this we will use the **sklearn** library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## and let's start with something we can know about ahead of time. \n",
    "## multivariate_normal( centre, direction of variation, number of points )\n",
    "mvn = np.random.multivariate_normal( [0,0], [[1,0], [0,5]], 4000 )\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(mvn)\n",
    "print(\"PCA components: \\n\",pca.components_, \"\\n\")\n",
    "print(\"Magnitude of PCA components: \\n\",pca.explained_variance_)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.axis([-9,9,-9,9])\n",
    "plt.scatter(*zip(*mvn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What PCA is doing\n",
    "\n",
    "Given $k$ *data points* in $\\mathbb R^n$ (the previous example has $n=2$ and $k=4000$)\n",
    "\n",
    "$$\\{ \\vec x_i \\in \\mathbb R^n : i = 1, 2, \\cdots, k\\}$$\n",
    "\n",
    "*Principal Component Analysis* is the process of constructing the matrix\n",
    "\n",
    "$$ A = \\left[ \\vec x_1, \\vec x_2, \\cdots, \\vec x_k \\right]$$\n",
    "\n",
    "(thinking of $\\mathbb R^n$ as consisting of column vectors). The $i$-th coordinate of the vector $\\vec x_j$ is called the $i$-th *feature* of the vector.  One then observes that the matrix\n",
    "\n",
    "$$ A \\cdot A^T $$\n",
    "\n",
    "is symmetric. Not only that, this is the matrix of dot products of the various *rows* of $A$. \n",
    "\n",
    "* * *\n",
    "\n",
    "**Theorem (Spectral Theorem)**: A symmetric $n \\times n$ matrix $X$ is diagonalizable, that is, there exists a real $n \\times n$ matrix $B$ such that\n",
    "$$B^{-1} X B$$\n",
    "is diagonal.  Moreover, the theorem tells us that one can assume that $B$ is *orthogonal*, $B^T B = I$. \n",
    "\n",
    "* * *\n",
    "\n",
    "The *PCA* method computes the matrix $B$ as well as the eigenvalues of $A \\cdot A^T$. PCA chooses the eigenvalues in *decreasing* order.  So the first column of the matrix $B$ represents the weighting of the features of the data with the highest amount of variance.  The second column vector represents the second-highest amount of variation among the features (technically the direction of the highest amount of variation once one projects the data to the orthogonal-complement of the first vector), etc. **PCA.components_** is the matrix $B$, while **PCA.explained\\_variance\\_** is the eigenvalues, in decreasing order. \n",
    "\n",
    "*Technical Issue* the **sklearn** library technically is substracting off the *mean* from every feature before it computes $B$, i.e. let\n",
    "\n",
    "$$\\tilde A = \\pmatrix{ \n",
    "a_{11} - avg_i\\{a_{1i}\\} & a_{12} - avg_i\\{a_{1i}\\} & \\cdots & a_{1k} - avg_i\\{a_{1i}\\} \\cr\n",
    "a_{21} - avg_i\\{a_{2i}\\} & a_{22} - avg_i\\{a_{2i}\\} & \\cdots & a_{2k} - avg_i\\{a_{2i}\\} \\cr\n",
    ". & . & & . \\cr\n",
    "a_{n1} - avg_i\\{a_{ni}\\} & a_{n2} - avg_i\\{a_{ni}\\} & \\cdots & a_{nk} - avg_i\\{a_{ni}\\} }$$\n",
    "\n",
    "The **sklearn** PCA algorithm replaces the matrix $A$ with $\\tilde A$ before computing $B$.\n",
    "\n",
    "*Fact*: Matrices of the form $A \\cdot A^T$ always have non-negative eigenvalues. \n",
    "\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us now look for elementary relationships in the crime data, of the sort PCA can see. \n",
    "\n",
    "Let's look for relations in crime occurances, by their types and the time of day in which they occur. \n",
    "\n",
    "Let's construct a list of vectors.  Our vectors will be in $\\mathbb R^n$ where $n = CT$.\n",
    "\n",
    "* $CT$ is the total number of crime types listed.  \n",
    "* $k$ the total number of crimes in the database.\n",
    "\n",
    "Each row of our matrix will represent a day.  And the column entries will be the number of crimes of various types on that day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list of crime types\n",
    "ctnl = []\n",
    "for a,b in vpd.ctypes.items():\n",
    "    for c in b:\n",
    "        ctnl.append((a,c))\n",
    "\n",
    "## reverse-lookup dictionary, to get the index of the crime type.\n",
    "rev_ctnl = dict([(ctnl[i], i) for i in range(len(ctnl))])\n",
    "\n",
    "## cdata dates as a set\n",
    "cdays = set([c.incident_datetime.date() for c in vpd.cdata])\n",
    "cdayl = list(cdays)\n",
    "\n",
    "## reverse-lookup a date\n",
    "rdaylook = dict([(cdayl[i], i) for i in range(len(cdayl))])\n",
    "\n",
    "A = np.zeros( (len(cdayl), len(ctnl)) )\n",
    "for c in vpd.cdata:\n",
    "    A[rdaylook[c.incident_datetime.date()], rev_ctnl[(c.parent_incident_type, c.incident_type_primary)]] += 1.0\n",
    "\n",
    "## build the data matrix. Every day will have a column consisting of the counts\n",
    "##  of the crime types on that day. \n",
    "\n",
    "pca = PCA(n_components=len(ctnl))\n",
    "pca.fit(A)\n",
    "\n",
    "C = pca.components_\n",
    "\n",
    "#print(\"PCA components: \\n\",pca.components_, \"\\n\")\n",
    "print(\"Magnitude of PCA components: \\n\",pca.explained_variance_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## takes as input the row number of the PCA analysis, and prints short string explaining\n",
    "## what it means\n",
    "def exp_row_pca(C, r):\n",
    "    ## list of entries w/index\n",
    "    Cl = [(100*C[r,i], i) for i in range(C.shape[1])]\n",
    "    Cs = sorted(Cl)\n",
    "    Cs.reverse()\n",
    "    Cp = [c for c in Cs if c[0]>0.0]\n",
    "    Cn = [c for c in Cs if c[0]<0.0]\n",
    "    Cn.reverse()\n",
    "    return (Cp, Cn)\n",
    "\n",
    "def text_corr(C, r):\n",
    "    Cp, Cn = exp_row_pca(C,r)\n",
    "    print(\"+corr: \")\n",
    "    for x in Cp:\n",
    "        if (x[0]>5.0):\n",
    "            print(\" \", ctnl[x[1]], \" pct %.1f\" % x[0])\n",
    "    print(\"-corr: \")\n",
    "    for x in Cn:\n",
    "        if (x[0]<-5.0):\n",
    "            print(\" \", ctnl[x[1]], \" pct %.1f\" % x[0])\n",
    "\n",
    "text_corr(C,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
